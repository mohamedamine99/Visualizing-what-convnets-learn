{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Visualizing what intermediate layers of the convnets learn\n",
        "\n",
        "\n",
        "This notebook demonstrates how visualizing convnet output or activations serves as a valuable tool in comprehending the manner by which successive convnet layers modify their input and helps provide an initial insight into the significance of individual convnet filters.\n"
      ],
      "metadata": {
        "id": "vcu3QvMVf-Tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "VLfFfPOXgaa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# file and directories manipulation\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# loading and building new models\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# image data manipulation\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "metadata": {
        "trusted": true,
        "id": "_dcjzFGDVwlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up working paths"
      ],
      "metadata": {
        "id": "JjxS_UAyain2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up paths\n",
        "CNN_model = '/kaggle/input/visualizing-what-convbets-learn/models/CNN cats vs dogs model.hdf5'\n",
        "test_imgs_path = '/kaggle/input/visualizing-what-convbets-learn/test images'\n",
        "\n",
        "cat_img = '/kaggle/input/visualizing-what-convbets-learn/test images/cat.PNG'\n",
        "dog_img = '/kaggle/input/visualizing-what-convbets-learn/test images/dog.PNG'\n",
        "\n",
        "two_cats_img = '/kaggle/input/visualizing-what-convbets-learn/test images/2 cats.PNG'\n",
        "two_dogs_img = '/kaggle/input/visualizing-what-convbets-learn/test images/2 dogs.PNG'\n",
        "\n",
        "cat_activations = '/kaggle/working/activations/cat'\n",
        "dog_activations = '/kaggle/working/activations/dog'\n",
        "\n",
        "two_cats_activations = '/kaggle/working/activations/two_cats'\n",
        "two_dogs_activations = '/kaggle/working/activations/two_dogs'\n",
        "\n",
        "cat_activations_2 = '/kaggle/working/activations_2/cat'\n",
        "dog_activations_2 = '/kaggle/working/activations_2/dog'\n",
        "\n",
        "two_cats_activations_2= '/kaggle/working/activations_2/two_cats'\n",
        "two_dogs_activations_2 = '/kaggle/working/activations_2/two_dogs'\n",
        "\n",
        "vid_activation = '/kaggle/working/vid_activation'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WDk1HAj4VwlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activation_dirs = [cat_activations, dog_activations, two_cats_activations, two_dogs_activations,\n",
        "                   cat_activations_2, dog_activations_2, two_cats_activations_2, two_dogs_activations_2, vid_activation ]\n",
        "\n",
        "# creating working directories\n",
        "for path in activation_dirs:\n",
        "    if not(os.path.exists(path)):\n",
        "        os.makedirs(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TTR3IeIFVwlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and testing CNN image classification model"
      ],
      "metadata": {
        "id": "7YBlshRcarQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the original CNN model\n",
        "model = load_model(CNN_model)\n",
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "qKu4opGhVwlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing and visualizing CNN model predictions\n",
        "labels = [\"cat\",\"dog\"]\n",
        "\n",
        "ncols = len(os.listdir(test_imgs_path))\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "for i , img_file in enumerate(os.listdir(test_imgs_path)):\n",
        "    print(img_file)\n",
        "    img = cv2.imread(os.path.join(test_imgs_path,img_file))\n",
        "    if (img is None):\n",
        "        continue\n",
        "    print(type(img))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    fig.add_subplot(1, ncols, i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    \n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = img / 255\n",
        "    img = np.reshape(img,(1,128,128,3))\n",
        "    \n",
        "    results = model.predict(img,verbose = 0)\n",
        "    results = np.squeeze(results)\n",
        "    plt.title(labels[np.round(results).astype(int)])\n",
        "\n",
        "    print(results)\n",
        "    "
      ],
      "metadata": {
        "trusted": true,
        "id": "bhulKRWMVwlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the new activations model"
      ],
      "metadata": {
        "id": "utzohznea7AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index of the flatten layer \n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(i,layer.name)\n",
        "    if layer.name.startswith(\"flatten\"):\n",
        "        break\n",
        "print(i)\n",
        "# the outputs of the activation model are the outputs of each layer of\n",
        "# the CNN model up until the flatten layer\n",
        "layers_outputs = [layer.output for layer in model.layers[:i]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "qS8DOzwmVwlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the new activations model\n",
        "activations_model = models.Model(inputs = model.input, outputs = layers_outputs)\n",
        "activations_model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ns121wKwVwlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extracting model activations"
      ],
      "metadata": {
        "id": "pw34oWg6bCyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_activations(img_path,model,verbose = False):\n",
        "    \n",
        "    #preprocess img\n",
        "    img = cv2.imread(img_path)\n",
        "    print(type(img))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img = cv2.resize(img,(128,128))\n",
        "    img = img / 255\n",
        "    img = np.reshape(img,(1,128,128,3))\n",
        "\n",
        "    # feed forward the preprocessed image to the activations model\n",
        "    activations = model.predict(img)\n",
        "\n",
        "    # display layer shapes\n",
        "    if verbose:\n",
        "        print(len(activations))\n",
        "        for i , activation in enumerate(activations) :\n",
        "            print(f' layer nÂ°{i} shape : {activation.shape}')\n",
        "    \n",
        "    return activations"
      ],
      "metadata": {
        "trusted": true,
        "id": "q2fKUfvHVwlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the intermediate layers activations for the cat_img\n",
        "activations = get_model_activations(cat_img,activations_model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uj3mWckwVwlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diplay the 4th feature map of the 1st layer\n",
        "plt.imshow(activations[0][0,:,:,4], cmap = 'viridis')"
      ],
      "metadata": {
        "trusted": true,
        "id": "DZHcovUuVwlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying results as a grid"
      ],
      "metadata": {
        "id": "_WYYYqMqcR56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_activations(img_pth ,activations_model , \n",
        "                        images_per_row = 16 , save_dir =None\n",
        "                       ):\n",
        "    \n",
        "    # get the list of activations for an image for each layer of the model\n",
        "    activations = get_model_activations(img_pth\n",
        "                                        ,activations_model)\n",
        "    # Get the index of the flatten layer \n",
        "    for i, layer in enumerate(model.layers):        \n",
        "        if layer.name.startswith(\"flatten\"):\n",
        "            break\n",
        "    # create a list of model layer names\n",
        "    layer_names = []\n",
        "    for idx,layer in enumerate(activations_model.layers[1:i+1]):\n",
        "        layer_names.append(f'{(idx +1):02d} {layer.name}')\n",
        "\n",
        "    # create a grid for each layer, and consisting of the layer activations/feature maps\n",
        "    for layer_name, layer_activation in zip(layer_names, activations):\n",
        "        n_features = layer_activation.shape[-1]\n",
        "        size = layer_activation.shape[1]\n",
        "        n_cols = n_features // images_per_row\n",
        "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "        for col in range(n_cols):\n",
        "            for row in range(images_per_row):\n",
        "                channel_image = layer_activation[0,\n",
        "                :, :,\n",
        "                col * images_per_row + row]\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "                display_grid[col * size : (col + 1) * size,\n",
        "                row * size : (row + 1) * size] = channel_image\n",
        "        scale = 1. / size\n",
        "        plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                            scale * display_grid.shape[0]))\n",
        "\n",
        "        plt.title(layer_name)\n",
        "        plt.grid(False)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # display the output grid for each layer\n",
        "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "\n",
        "        # save the grid for each layer in save_dir directory as .png format\n",
        "        if save_dir is not None:\n",
        "            if not os.path.isdir(save_dir):\n",
        "                print(f\"Creating {save_dir} ... \")\n",
        "                os.makedirs(save_dir)\n",
        "            \n",
        "            plt.savefig(f'{save_dir}/{layer_name}.png')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "F4ZAb925VwlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display and save activations for cat_img\n",
        "display_activations(img_pth = cat_img ,activations_model = activations_model\n",
        "                    , images_per_row = 16 , save_dir = cat_activations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "iYJJBnZ-VwlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# centers images of size (1600,1600) inside a canvas of the same size, used on the generated activations grids\n",
        "def center_imgs(imgs_dir):\n",
        "    for i,filename in enumerate(os.listdir(imgs_dir)):\n",
        "        img = 255 * np.ones((1600,1600,3))\n",
        "        fig = cv2.imread(os.path.join(imgs_dir,filename))\n",
        "        img[(img.shape[0] // 2 ) - fig.shape[0] // 2:  (img.shape[0] // 2 ) + fig.shape[0] // 2] = fig\n",
        "        \n",
        "        text = f'{filename[:-4]}'\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        text_scale = 2.75\n",
        "        text_thickness = 8\n",
        "        text_size = cv2.getTextSize(text, font, text_scale, text_thickness)[0]\n",
        "        height, width, _ = img.shape\n",
        "        \n",
        "        x = int((width - text_size[0]) / 2)\n",
        "        \n",
        "        img = cv2.putText(img,text,(x,150),font, text_scale, (0, 0, 255), text_thickness, cv2.LINE_AA)\n",
        "        cv2.imwrite(f\"{imgs_dir}/{filename}\", img)\n",
        "    \n"
      ],
      "metadata": {
        "trusted": true,
        "id": "qvjhDoAQVwlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "center_imgs(cat_activations)\n",
        "plt.imshow(imageio.imread(os.path.join(cat_activations,os.listdir(cat_activations)[3])))"
      ],
      "metadata": {
        "trusted": true,
        "id": "k4dt65O5VwlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/kaggle/working/activations')))\n",
        "print(os.listdir(cat_activations))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "leROxbizVwlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates GIFs from images located within a directory\n",
        "def create_gif(source_path,save_dir,fps = 0.5):\n",
        "    with imageio.get_writer(save_dir, mode='I', fps = fps) as writer:\n",
        "        for filename in sorted(os.listdir(source_path)):\n",
        "            #print(filename,end =' - ')\n",
        "            image = imageio.imread(os.path.join(source_path,filename))\n",
        "            writer.append_data(image)"
      ],
      "metadata": {
        "trusted": true,
        "id": "a-vStFuiVwlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a GIF for the activations of cat_img\n",
        "create_gif(cat_activations,'cat activations.gif',fps = 0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BXROgLOxVwlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display and save activations for dog_img\n",
        "display_activations(img_pth = dog_img ,activations_model = activations_model\n",
        "                    , images_per_row = 16 , save_dir = dog_activations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TcgXLURDVwlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display and save activations for two_cats_img\n",
        "display_activations(img_pth = two_cats_img ,activations_model = activations_model\n",
        "                    , images_per_row = 16 , save_dir = two_cats_activations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ETCQFsgdVwlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display and save activations for two_dogs_img\n",
        "display_activations(img_pth = two_dogs_img ,activations_model = activations_model\n",
        "                    , images_per_row = 16 , save_dir = two_dogs_activations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DsOa970tVwlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# centering and adding titles to the grid activations\n",
        "center_imgs(dog_activations)\n",
        "center_imgs(two_dogs_activations)\n",
        "center_imgs(two_cats_activations)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1A2-wFaNVwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating GIFs for the grid activations for each of the test imgs\n",
        "create_gif(dog_activations,'dog activations.gif',fps = 0.5)\n",
        "create_gif(two_cats_activations,'two cats activations.gif',fps = 0.5)\n",
        "create_gif(two_dogs_activations,'two dogs activations.gif',fps = 0.5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ul3luraCVwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying and saving results v2"
      ],
      "metadata": {
        "id": "KcFAS9yXceHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusts a text in an image\n",
        "def center_text(img,text='', font_scale = 0.5 ,offset = (0,0)):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    \n",
        "\n",
        "    # Get the size of the text string\n",
        "    (text_width, text_height) = cv2.getTextSize(text, font, font_scale, thickness=2)[0]\n",
        "\n",
        "    # Calculate the x, y coordinates for the text\n",
        "    x = int((img.shape[1] - text_width) / 2)\n",
        "    y = text_height + 5\n",
        "\n",
        "    img = cv2.putText(img,text,(x + offset[0],y + offset[1]),\n",
        "                      font, font_scale, (0, 0, 255),\n",
        "                      1, cv2.LINE_AA)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "adzx4yY4VwlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save each activation for each layer individually of a certain img\n",
        "def save_conv_activations(img_path, model ,save_dir ):\n",
        "    # save only the conv layers indices in a list\n",
        "\n",
        "    conv_idxs = [i for i,layer in enumerate(model.layers) if \"conv\" in layer.name]\n",
        "    print(conv_idxs)\n",
        "    activations = get_model_activations(img_path,model)\n",
        "    print(conv_idxs)\n",
        "    for layer in conv_idxs:\n",
        "        for feature_map in range(activations[layer].shape[3]):\n",
        "            activation = activations[layer][0,:,:,feature_map]\n",
        "\n",
        "            # Normalize the grayscale image\n",
        "            normalized = cv2.normalize(activation, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "            # Apply viridis colormap\n",
        "            viridis = cv2.applyColorMap(normalized, cv2.COLORMAP_VIRIDIS)\n",
        "            im_size = viridis.shape[0]\n",
        "            viridis = cv2.resize(viridis,(256,256))\n",
        "\n",
        "\n",
        "            # border to add to the image in order to expand it\n",
        "            border_width = 75\n",
        "            border_color = [255, 255, 255]  # white color in BGR format\n",
        "\n",
        "            # Add border to the image\n",
        "            viridis = cv2.copyMakeBorder(viridis, border_width, border_width, \n",
        "                                         border_width, border_width, cv2.BORDER_CONSTANT, \n",
        "                                         value=border_color)\n",
        "\n",
        "            # Get the size of the text string\n",
        "            text1 = \"Size -- Layer idx -- Feature map idx\"\n",
        "            text2=f'({im_size}x{im_size})--{layer:02d}--{feature_map:03d}'\n",
        "\n",
        "            # add adjusted text\n",
        "            center_text(viridis,text1)\n",
        "            center_text(viridis,text2,font_scale = 1 ,offset = (0,40))\n",
        "\n",
        "            cv2.imwrite(f'{save_dir}/{layer:02d}--{feature_map:03d}.png',viridis)\n",
        "    "
      ],
      "metadata": {
        "trusted": true,
        "id": "daKbv-_wVwlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save each activation for each layer individually of a cat_img\n",
        "save_conv_activations(cat_img, activations_model , \n",
        "                      cat_activations_2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UhnxQ-u7VwlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a GIF for each activation for each layer individually of a cat_img\n",
        "create_gif(cat_activations_2,'cat activations 2.gif',fps = 5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nW0SA6npVwlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_conv_activations(dog_img, activations_model , \n",
        "                      dog_activations_2)\n",
        "\n",
        "save_conv_activations(two_dogs_img, activations_model , \n",
        "                      two_dogs_activations_2)\n",
        "\n",
        "save_conv_activations(two_cats_img, activations_model , \n",
        "                      two_cats_activations_2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TDbK7iXyVwlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_gif(dog_activations_2,'dog_activations_2.gif',fps = 5)\n",
        "create_gif(two_dogs_activations_2,'two_dogs_activations_2.gif',fps = 5)\n",
        "create_gif(two_cats_activations_2,'two_cats_activations_2.gif',fps = 5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "B97SJrMBVwlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving all results in a zip file"
      ],
      "metadata": {
        "id": "8rjsFvvIbNNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save results to a results.zip file for later download\n",
        "!zip -r results.zip /kaggle/working"
      ],
      "metadata": {
        "trusted": true,
        "id": "rcw39m5BVwlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}